# Sentiment Analysis - Comprehensive Homework Solutions

This repository contains a complete implementation of sentiment analysis algorithms and comprehensive homework solutions covering multiple machine learning approaches for text classification.

## ğŸ“Š Project Overview

This project demonstrates a thorough understanding of sentiment analysis through:
- **Custom logistic regression implementation** from scratch
- **Comparative analysis** with scikit-learn implementations  
- **Feature engineering** and normalization techniques
- **Numerical stability** solutions for gradient descent
- **Advanced ML model comparisons** with 17 different algorithms
- **Data preprocessing** and scaling techniques

## ğŸ“‚ Project Structure 
```
Sentiment_analysis/
â”œâ”€â”€ sentiment_analysis_homework_solutions.ipynb  # Complete homework solutions
â”œâ”€â”€ pyproject.toml                              # Project configuration and dependencies
â”œâ”€â”€ uv.lock                                     # Lock file for dependencies
â”œâ”€â”€ README.md                                   # This documentation
â”œâ”€â”€ config/                                     # Configuration files for training
â”œâ”€â”€ dataset/                                    # Data directory
â”‚   â”œâ”€â”€ raw/                                   # Raw datasets
â”‚   â”œâ”€â”€ processed/                             # Processed datasets
â”‚   â””â”€â”€ README.md                              # Dataset descriptions
â””â”€â”€ src/                                       # Source code modules
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ data/                                  # Data processing utilities
    â”œâ”€â”€ models/                                # Model definitions
    â”œâ”€â”€ training/                              # Training logic
    â”œâ”€â”€ evaluation/                            # Model evaluation
    â””â”€â”€ utils/                                 # Common utilities
```

## ğŸ¯ Assignment Solutions Overview

### **Exercise 1: Custom vs Scikit-learn Logistic Regression**
- âœ… Implemented custom logistic regression from scratch
- âœ… Comprehensive comparison with sklearn implementations
- âœ… Performance analysis across multiple configurations
- **Result**: Custom implementation achieved **99.60%** accuracy

### **Exercise 2: Gradient Descent Numerical Stability**  
- âœ… Identified and analyzed overflow issues in high-iteration training
- âœ… Implemented numerically stable sigmoid and cost functions
- âœ… Added gradient clipping and early stopping mechanisms
- **Result**: Successfully trained with 100K iterations without errors

### **Exercise 3: Feature Normalization with Sentence Length**
- âœ… Implemented normalization by N = train_set_length Ã— sentence_length
- âœ… Statistical comparison of feature distributions
- âœ… Impact analysis on model performance
- **Result**: Original features performed better (**99.50%** vs **96.40%**)

### **Exercise 4: Feature Scaling Techniques**
- âœ… Tested MinMax, Standard, and Robust scaling methods
- âœ… Visual comparison of feature distributions
- âœ… Performance impact analysis
- **Result**: Standard scaling achieved best performance (**99.55%**)

### **Exercise 5: Alternative Decision Functions**
- âœ… Implemented simple frequency-based classifier
- âœ… Detailed comparison with logistic regression
- âœ… Disagreement analysis between methods
- **Result**: Simple rule achieved **99.65%** accuracy

### **Exercise 7: Comprehensive ML Model Comparison**
- âœ… Tested 17 different ML algorithms including:
  - Logistic Regression variants
  - Random Forest and Decision Trees
  - SVM with different kernels
  - Gradient Boosting and AdaBoost
  - K-Nearest Neighbors
  - Naive Bayes variants
  - Neural Networks (MLP)
- âœ… Performance metrics analysis (Accuracy, Precision, Recall, F1-Score)
- âœ… Model complexity vs performance visualization
- **Result**: Ridge Classifier achieved best accuracy (**99.61%**)

## ğŸ“ˆ Key Performance Results

| Method | Accuracy | Precision | Recall | F1-Score |
|--------|----------|-----------|---------|----------|
| **Custom Logistic Regression** | **99.60%** | 99.20% | 99.80% | 99.50% |
| Sklearn Logistic Regression | 99.50% | 99.20% | 99.80% | 99.50% |
| Simple Frequency Classifier | 99.65% | N/A | N/A | N/A |
| **Best ML Model (Ridge)** | **99.61%** | 98.84% | 100% | 99.41% |
| Random Forest (Tuned) | 99.50% | 99.40% | 99.50% | 99.44% |
| Neural Network (Large) | 99.00% | 98.00% | 100% | 99.00% |

## ğŸ”§ Technical Implementation Highlights

### **Custom Logistic Regression Features:**
- Sigmoid activation with numerical stability
- Gradient descent optimization 
- Cost function monitoring
- Custom training and prediction methods

### **Numerical Stability Solutions:**
- Epsilon clipping for log computations
- Z-value clipping for sigmoid overflow prevention
- Gradient clipping for training stability
- Early stopping mechanisms

### **Feature Engineering:**
- Frequency-based feature extraction
- Sentence length normalization
- Multiple scaling techniques
- Statistical analysis and visualization

### **Comprehensive Analysis:**
- Model comparison matrices
- Performance visualization charts
- Training time analysis
- Complexity vs performance trade-offs

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- Jupyter Notebook
- Required packages: numpy, pandas, scikit-learn, matplotlib, seaborn, nltk

### Installation
```bash
# Clone the repository
git clone https://github.com/Duongvu05/Sentiment_analysis.git
cd Sentiment_analysis

# Install dependencies using uv (recommended)
uv sync

# Or install with pip
pip install -e .
```

### Running the Homework Solutions
```bash
# Open the comprehensive solutions notebook
jupyter notebook sentiment_analysis_homework_solutions.ipynb
```

## ğŸ“Š Visualizations and Analysis

The notebook includes comprehensive visualizations:
- **Performance comparison charts** across all methods
- **Feature distribution histograms** for different scaling techniques
- **Model complexity analysis** plots
- **Training time comparisons**
- **Confusion matrices** and error analysis
- **Weight comparison visualizations**

## ğŸ¯ Key Learnings and Insights

1. **Custom implementations can compete with library solutions** when properly optimized
2. **Numerical stability is crucial** for high-iteration training scenarios
3. **Feature scaling significantly impacts** certain algorithms more than others
4. **Simple rule-based approaches** can be surprisingly effective for well-structured problems
5. **Advanced ensemble methods** don't always outperform simpler approaches on clean datasets
6. **Proper evaluation and comparison methodology** is essential for reliable results

## ğŸ¤ Contributing

Feel free to submit issues and enhancement requests!

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---
*This project demonstrates comprehensive understanding of machine learning fundamentals, from custom algorithm implementation to advanced model comparison and analysis.*
